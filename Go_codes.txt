- inatall Go lang

sudo apt update
sudo apt install golang-go -y
go version

- Golang directory
mkdir go_code && cd go_code
go mod init go_kafka_code

- Download kafka plugin for go
go get github.com/segmentio/kafka-go

- Create a file and put the code where golang sends msgs to kafka
vim send_to_kafka.go
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"runtime"
	"time"

	"github.com/segmentio/kafka-go"
)

type Metrics struct {
	Timestamp string  `json:"timestamp"`
	Node      string  `json:"node"`
	MemAlloc  uint64  `json:"mem_alloc_mb"`
	MemTotal  uint64  `json:"mem_total_mb"`
	System    string  `json:"os"`
}

func main() {
	// 1. Setup Kafka connection
	brokers := []string{"kafka01:9092", "kafka02:9092", "kafka03:9092"}
	topic := "golang_msgs"
	host, _ := os.Hostname()

	writer := &kafka.Writer{
		Addr:     kafka.TCP(brokers...),
		Topic:    topic,
		Balancer: &kafka.LeastBytes{},
	}
	defer writer.Close()

	fmt.Printf("Monitoring started on %s. Sending to Kafka every 10s...\n", host)

	// 2. Continuous Loop
	for {
		var m runtime.MemStats
		runtime.ReadMemStats(&m)

		data := Metrics{
			Timestamp: time.Now().Format(time.RFC3339),
			Node:      host,
			MemAlloc:  m.Alloc / 1024 / 1024,
			MemTotal:  m.Sys / 1024 / 1024, // Total memory requested from OS
			System:    runtime.GOOS,
		}

		payload, _ := json.Marshal(data)

		// 3. Write to Kafka
		err := writer.WriteMessages(context.Background(),
			kafka.Message{Key: []byte(host), Value: payload},
		)

		if err != nil {
			log.Printf("Kafka Error: %v", err)
		}

		time.Sleep(10 * time.Second)
	}
}

- Check if it is running proper & Debug
go run send_to_kafka.go

- Build the go code to a binary executable
go build -o send_to_kafka send_to_kafka.go

- move to bin folder
sudo mv send_to_kafka /usr/local/bin/

- create a service for that binary
[Unit]
Description=Kafka System Metrics Producer
After=network.target

[Service]
User=kafka
Group=kafka
WorkingDirectory=/usr/local/bin
ExecStart=/usr/local/bin/send_to_kafka

# Automatically restart if the app crashes
Restart=always
RestartSec=5

# LOG MANAGEMENT:
# We send logs to the system journal instead of a file
StandardOutput=journal
StandardError=journal
# Give the logs a specific name so we can find them
SyslogIdentifier=send_to_kafka

[Install]
WantedBy=multi-user.target


- do the same for consumer_code
package main

package main

import (
	"context"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"strconv"
	"time"

	"github.com/segmentio/kafka-go"
)

type Metrics struct {
	Timestamp string `json:"timestamp"`
	Node      string `json:"node"`
	MemAlloc  uint64 `json:"mem_alloc_mb"`
	MemTotal  uint64 `json:"mem_total_mb"`
}

func main() {
	topic := "golang_msgs"
	brokers := []string{"kafka01:9092", "kafka02:9092", "kafka03:9092"}
	
	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers: brokers,
		Topic:   topic,
		GroupID: "grafana-test-group",
	})
	defer reader.Close()

	startTime := time.Now()

	for {
		// Every 4 minutes, we switch between "Lagging" and "Fast"
		elapsed := time.Since(startTime).Minutes()
		isSlowMode := int(elapsed)%4 < 2 // First 2 mins slow, next 2 mins fast

		batchSize := 30
		var batch []kafka.Message

		// Collect 30 messages
		for len(batch) < batchSize {
			m, err := reader.FetchMessage(context.Background())
			if err != nil {
				break
			}
			batch = append(batch, m)
		}

		// Write to CSV
		file, _ := os.OpenFile("metrics_data.csv", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
		csvWriter := csv.NewWriter(file)
		for _, msg := range batch {
			var m Metrics
			json.Unmarshal(msg.Value, &m)
			csvWriter.Write([]string{m.Timestamp, m.Node, strconv.FormatUint(m.MemAlloc, 10)})
		}
		csvWriter.Flush()
		file.Close()

		// Commit progress to Kafka
		reader.CommitMessages(context.Background(), batch...)

		if isSlowMode {
			fmt.Println(">>> SLOW MODE: Creating Lag... (Sleeping 20s)")
			time.Sleep(20 * time.Second)
		} else {
			fmt.Println(">>> FAST MODE: Clearing Lag... (No Sleep)")
			// No sleep here, it will loop back and grab the next 30 immediately
		}
	}
}
